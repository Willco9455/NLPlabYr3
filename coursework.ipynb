{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework Start \n",
    "This is my coursework wow how amazing that is increible\n",
    "\n",
    "Insatlls: \n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install -q wordcloud\n",
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import string\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import spatial\n",
    "import numpy \n",
    "\n",
    "# for Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import wordcloud\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "# defines  lemmatizer for use\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Bag of Words with tf*idf\n",
    "\n",
    "Current best accuracy from validation dataset provided is 58.88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willmoran/Documents/UNI/NLP_coursework/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# basic Tokenisation function, removes all puntuation apart from - and splits on spaces whilst replacing uppercase with lower \n",
    "def tokenizeText(text):\n",
    "  # Remove punctuation except apostrophes\n",
    "  translator = str.maketrans('', '', (string.punctuation.replace(\"-\", \"\")))\n",
    "  text = text.translate(translator).lower()\n",
    "\n",
    "  # Splits on spaces \n",
    "  tokens = text.split()\n",
    "\n",
    "  # lemmatizes each token \n",
    "  for i in range(0, len(tokens)):\n",
    "    tokens[i] = lemmatizer.lemmatize(tokens[i])\n",
    "\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def getDocTermVecs(corpus):\n",
    "  # uses sklearn function to transform a corpus into a document-term matrix whilst also tokenizing using my tokenizeText function\n",
    "  vectorizer = TfidfVectorizer(tokenizer=tokenizeText) \n",
    "  X = vectorizer.fit_transform(corpus)\n",
    "  \n",
    "  #gets relevent terms in correct order from doc-term matrix \n",
    "  terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "  # returning in form terms, vecor representations\n",
    "  return terms.tolist(), X.transpose().toarray()\n",
    "\n",
    "# returns corpus, corpusTokenized\n",
    "def extractCorpus(corpusDest):\n",
    "  # gets all descriptions from training set \n",
    "  corpus = []\n",
    "  # corpusTokenized used for word2vec later\n",
    "  corpusTokenized = []\n",
    "  labels = []\n",
    "  with open(corpusDest, newline='') as trainingData:\n",
    "    for row in csv.reader(trainingData):\n",
    "      if row[3] == 'comedy': continue\n",
    "      tokenizedScen = tokenizeText(row[2])\n",
    "      corpusTokenized.append(tokenizedScen)\n",
    "      corpus.append(row[2])\n",
    "      labels.append([int(row[3]),int(row[4]),int(row[5]),int(row[6]),\n",
    "                     int(row[7]),int(row[8]),int(row[9]),int(row[10]),int(row[11])])\n",
    "      # labels.append(1)\n",
    "  \n",
    "  return corpus, corpusTokenized, labels\n",
    "\n",
    "corpus, corpusTokenized, labels = extractCorpus('./data/Training-dataset.csv')\n",
    "# generates docTermMatrix from corpus \n",
    "terms, vecMatrix = getDocTermVecs(corpus)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gets the average feature vector \n",
    "avgVect = numpy.mean(vecMatrix, axis=0)\n",
    "\n",
    "# calculates the cosine similarity between two words for task 1 bag of words\n",
    "def cosineSimilarityBoW(term1, term2):\n",
    "  \n",
    "  # gets each term vector from the vector matrix, if a term is not in the matrix it is given the average vector matrix\n",
    "  # (this seemed to improve performace by 1-3% accuracy)\n",
    "  if (term1 in terms) and (term2 in terms):\n",
    "    term1Vec = vecMatrix[terms.index(term1)]\n",
    "    term2Vec = vecMatrix[terms.index(term2)]\n",
    "  elif (term1 in terms) and not(term2 in terms):\n",
    "    term1Vec = vecMatrix[terms.index(term1)]\n",
    "    term2Vec = avgVect\n",
    "  elif not(term1 in terms) and (term2 in terms): \n",
    "    term1Vec = avgVect\n",
    "    term2Vec = vecMatrix[terms.index(term1)]\n",
    "  else:\n",
    "    term1Vec, term2Vec = avgVect\n",
    "  \n",
    "\n",
    "  result = 1 - spatial.distance.cosine(term1Vec, term2Vec)\n",
    "  return result\n",
    "\n",
    "def generateTask1Results(cosineSimFunction):\n",
    "  # reads terms from validation dataset -> calculates results and stores results in results array\n",
    "  results = []\n",
    "  with open('./data/Task-1-validation-dataset.csv', newline='') as validationDataset:\n",
    "    for row in csv.reader(validationDataset):\n",
    "      # calculates the cosine similarity for the lematized version of each pair of words in the validation dataset\n",
    "      cosineSim = cosineSimFunction(\n",
    "        lemmatizer.lemmatize(row[1]), \n",
    "        lemmatizer.lemmatize(row[2]))\n",
    "      \n",
    "      # adds results with corrosponding pairID to results array\n",
    "      resultsRow = [row[0],cosineSim]\n",
    "      results.append(resultsRow)\n",
    "\n",
    "  # writes the results to results csv\n",
    "  with open('./data/Task-1-validation-dataset-results.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerows(results)  \n",
    "\n",
    "generateTask1Results(cosineSimilarityBoW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 - Create word-vector respresentation (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=corpusTokenized, min_count = 1, vector_size = 500,\n",
    "                                             window = 7, sg = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "def cosineSimilarityW2V(term1, term2):\n",
    "  try: \n",
    "    return model.wv.similarity(term1, term2)\n",
    "  except:\n",
    "    return 0.01\n",
    "\n",
    "print(model.wv.get_vector('room').shape)\n",
    "\n",
    "generateTask1Results(cosineSimilarityW2V)\n",
    "# print(model.wv.similarity())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2.1 Classification with SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lankester', 'merrin', 'is', 'a', 'veteran', 'catholic', 'priest', 'and', 'exorcist', 'who', 'is', 'on', 'an', 'archaeological', 'dig', 'in', 'iraq', 'there', 'he', 'find', 'an', 'amulet', 'that', 'resembles', 'the', 'statue', 'of', 'pazuzu', 'a', 'demon', 'whom', 'merrin', 'had', 'defeated', 'year', 'before', 'merrin', 'then', 'realizes', 'the', 'demon', 'ha', 'returned', 'to', 'seek', 'revenge', 'meanwhile', 'in', 'georgetown', 'actress', 'chris', 'macneil', 'is', 'living', 'on', 'location', 'with', 'her', '12-year-old', 'daughter', 'regan', 'where', 'chris', 'ha', 'just', 'wrapped', 'the', 'final', 'scene', 'of', 'a', 'film', 'about', 'student', 'activism', 'directed', 'by', 'her', 'friend', 'and', 'associate', 'burke', 'dennings', 'after', 'playing', 'with', 'a', 'ouija', 'board', 'and', 'contacting', 'a', 'supposedly', 'imaginary', 'friend', 'whom', 'she', 'call', 'captain', 'howdy', 'regan', 'begin', 'acting', 'strangely', 'including', 'making', 'mysterious', 'noise', 'stealing', 'constantly', 'using', 'obscene', 'language', 'and', 'exhibiting', 'abnormal', 'strength', 'chris', 'host', 'a', 'party', 'only', 'for', 'regan', 'to', 'come', 'downstairs', 'unannounced', 'telling', 'one', 'of', 'the', 'guest', 'who', 'is', 'an', 'astronaut', 'youre', 'gonna', 'die', 'up', 'there', 'and', 'then', 'urinating', 'on', 'the', 'floor', 'regans', 'bed', 'also', 'begin', 'to', 'shake', 'violently', 'much', 'to', 'her', 'and', 'her', 'mother', 'horror', 'chris', 'consults', 'multiple', 'physician', 'but', 'dr', 'klein', 'and', 'his', 'associate', 'find', 'nothing', 'medically', 'wrong', 'with', 'her', 'daughter', 'despite', 'regan', 'undergoing', 'a', 'battery', 'of', 'diagnostic', 'test', 'one', 'night', 'when', 'chris', 'is', 'out', 'burke', 'dennings', 'is', 'babysitting', 'regan', 'only', 'for', 'chris', 'to', 'come', 'home', 'to', 'hear', 'he', 'ha', 'died', 'falling', 'out', 'the', 'window', 'although', 'this', 'is', 'assumed', 'to', 'have', 'been', 'an', 'accident', 'given', 'burke', 'history', 'of', 'heavy', 'drinking', 'his', 'death', 'is', 'investigated', 'by', 'lieutenant', 'william', 'kinderman', 'who', 'interview', 'chris', 'a', 'well', 'a', 'priest', 'and', 'psychiatrist', 'father', 'damien', 'karras', 'who', 'ha', 'had', 'his', 'faith', 'in', 'god', 'severely', 'weakened', 'and', 'left', 'badly', 'shaken', 'after', 'the', 'death', 'of', 'his', 'frail', 'mother', 'the', 'doctor', 'thinking', 'that', 'regan', 'only', 'belief', 'she', 'is', 'possessed', 'recommend', 'an', 'exorcism', 'to', 'be', 'performed', 'chris', 'arranges', 'a', 'meeting', 'with', 'karras', 'after', 'recording', 'regan', 'speaking', 'backwards', 'and', 'witnessing', 'the', 'etching', 'of', 'the', 'word', 'help', 'me', 'on', 'her', 'stomach', 'karras', 'is', 'convinced', 'regan', 'is', 'possessed', 'believing', 'her', 'soul', 'is', 'in', 'danger', 'he', 'decides', 'to', 'perform', 'an', 'exorcism', 'the', 'experienced', 'merrin', 'is', 'selected', 'to', 'do', 'so', 'instead', 'with', 'karras', 'present', 'to', 'assist', 'both', 'priest', 'witness', 'regan', 'perform', 'a', 'series', 'of', 'bizarre', 'vulgar', 'act', 'and', 'confine', 'her', 'to', 'her', 'bedroom', 'they', 'attempt', 'to', 'exorcise', 'the', 'demon', 'but', 'a', 'stubborn', 'pazuzu', 'toy', 'with', 'them', 'especially', 'karras', 'karras', 'show', 'weakness', 'and', 'is', 'dismissed', 'by', 'merrin', 'who', 'attempt', 'the', 'exorcism', 'alone', 'karras', 'enters', 'the', 'room', 'and', 'discovers', 'merrin', 'ha', 'died', 'of', 'a', 'heart', 'attack', 'after', 'failing', 'to', 'revive', 'merrin', 'the', 'enraged', 'karras', 'confronts', 'the', 'mocking', 'laughing', 'spirit', 'of', 'pazuzu', 'tackling', 'the', 'demon', 'to', 'the', 'ground', 'at', 'karrass', 'furious', 'demand', 'pazuzu', 'then', 'posse', 'karras', 'leaving', 'regans', 'body', 'in', 'a', 'moment', 'of', 'self-sacrifice', 'the', 'priest', 'throw', 'himself', 'out', 'of', 'the', 'window', 'without', 'allowing', 'pazuzu', 'to', 'compel', 'him', 'to', 'harm', 'regan', 'and', 'is', 'himself', 'mortally', 'injured', 'father', 'dyer', 'an', 'old', 'friend', 'of', 'karras', 'happens', 'upon', 'the', 'scene', 'and', 'administers', 'the', 'last', 'rite', 'to', 'his', 'friend', 'a', 'few', 'day', 'later', 'regan', 'who', 'is', 'now', 'back', 'to', 'her', 'normal', 'self', 'prepares', 'to', 'leave', 'for', 'los', 'angeles', 'with', 'her', 'mother', 'although', 'regan', 'ha', 'no', 'apparent', 'recollection', 'of', 'her', 'possession', 'she', 'give', 'father', 'dyer', 'a', 'kiss', 'a', 'likely', 'hint', 'at', 'posthumous', 'thanks', 'to', 'father', 'merrin', 'and', 'father', 'karras', 'kinderman', 'who', 'narrowly', 'miss', 'their', 'departure', 'befriends', 'father', 'dyer', 'a', 'he', 'investigates', 'karrass', 'death']\n"
     ]
    }
   ],
   "source": [
    "print(corpusTokenized[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# VOCAB_SIZE = 1000\n",
    "\n",
    "encoder = tf.keras.layers.TextVectorization()\n",
    "encoder.adapt(corpus)\n",
    "vocab = numpy.array(encoder.get_vocabulary())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  6/259 [..............................] - ETA: 22:16 - loss: 4.3587 - accuracy: 0.0781"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(350)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),  # Adding dropout for regularization\n",
    "    # tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(9, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.legacy.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# labelsVectorised = numpy.asarray(labels).astype('float32').reshape((-1,1))\n",
    "history = model.fit(corpus, labels, epochs=5,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1420dd6c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1420dd6c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 860ms/step\n",
      "[[0.05772711 0.08939094 0.10727655 0.01113144 0.3150721  0.08923353\n",
      "  0.11422064 0.00992511 0.20602252]]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(['''In the crime-ridden city of Tremont, renowned investigative journalist, Amelia Williams, wages war against an influential criminal syndicate. The Syndicate, led by the ruthless mafia boss, Victor Russo, smuggles drugs and extorts businesses, instilling fear throughout the entire city. Amidst the chaos, Amelia's investigative reporting catches the attention of Jared Bennett, a former Special Forces operative haunted by his personal demons.\n",
    "\n",
    "Amelia, driven by her desire for justice, covertly gathers vital evidence against the Syndicate, documenting their illegal activities in a clandestine exposé. However, her secret investigation is abruptly cut short when Amelia is found brutally murdered in her apartment. The loss devastates her loyal intern, Julia Evans, who becomes determined to avenge her mentor's death and complete the mission Amelia started.\n",
    "\n",
    "Upon hearing the tragic news, Jared, who had developed feelings for Amelia, is consumed by a thirst for revenge. Haunted by his past failures, Jared pledges to help Julia expose the Syndicate and bring justice to Amelia's killers. With their hearts set on vengeance, they form an unlikely alliance, setting in motion a thrilling tale of violence, romance, and redemption.\n",
    "\n",
    "As Jared and Julia delve deeper into the treacherous underworld of Tremont, they discover an unexpected ally in the form of Anthony Marino, an undercover cop who has infiltrated the Syndicate. Together, the trio infiltrates a high-profile underground casino run by the Syndicate, hoping to find evidence to dismantle Russo's empire. Amidst the chaos of the casino, Jared crosses paths with Victoria Russo, Victor's estranged daughter. Their connection is immediate, their attraction undeniable, but Victoria is torn between her loyalty to her father and the sense of justice that Jared represents.\n",
    "\n",
    "As the investigation intensifies, the Syndicate grows suspicious, and Victor Russo takes drastic measures to protect his criminal empire. He assigns his merciless enforcer, Dimitri Saxon, to track down and eliminate the threats. Saxon, a lethal hitman with his own dark past, is relentless in his pursuit of Jared, Julia, and Anthony, leaving a path of violence in his wake.\n",
    "\n",
    "In a climactic battle, the trio confronts the Syndicate at an abandoned warehouse, pitting their wits, skills, and firepower against Russo's formidable criminal organization. Victoria makes a fateful decision, siding with Jared and Julia, igniting a passionate battle for redemption, love, and justice.\n",
    "\n",
    "Amidst the explosive conflict, long-buried secrets are unearthed, shedding light on the tragic events that brought Jared, Amelia, and Victoria to this final confrontation. The truth becomes a vengeful weapon, empowering our heroes to reveal the Syndicate's darkest secrets and dismantle its oppressive grip on Tremont.\n",
    "\n",
    "In an emotional and action-packed finale, Jared confronts Victor Russo, facing not only the man responsible for Amelia's death but also his own guilt over past failures. The battle ends with the triumph of justice, as Russo meets his demise, and Julia finally avenges her beloved mentor.\n",
    "\n",
    "With the Syndicate dismantled, Tremont begins to heal, and the remaining collaborators face the consequences of their actions. As the dust settles, Jared and Victoria, having survived the storm, find solace and love in each other's arms. Together, they commit to rebuilding their lives, forever changed by their shared journey of heartbreak, revenge, and ultimately, redemption.\n",
    "'''])\n",
    "print(prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
